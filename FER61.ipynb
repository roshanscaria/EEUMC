{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hnPENGpxvDl",
        "outputId": "6770652c-f45d-45a6-cfef-05384b81a705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "   emotion                                             pixels     Usage\n",
            "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
            "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
            "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
            "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
            "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the CSV file in your Google Drive\n",
        "drive_csv_path = '/content/drive/My Drive/fer2013.csv'\n",
        "\n",
        "# Read CSV file into DataFrame\n",
        "df = pd.read_csv(drive_csv_path)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "NC-2kFLc0cEY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.emotion.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qp8T544y8LS",
        "outputId": "15dae6a2-6d55-4b02-e5f1-8adabd410aef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 4, 6, 3, 5, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_to_text = {\n",
        "    0: 'anger',\n",
        "    1: 'disgust',\n",
        "    2: 'fear',\n",
        "    3: 'happiness',\n",
        "    4: 'sadness',\n",
        "    5: 'surprise',\n",
        "    6: 'neutral'\n",
        "}\n"
      ],
      "metadata": {
        "id": "N7wJQpSezDW-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.array(df.pixels.loc[0].split(' ')).reshape(48,48)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eN-C_ViUzWPx",
        "outputId": "c84d439c-0086-4b43-f0e2-d494f649ded4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['70', '80', '82', ..., '52', '43', '41'],\n",
              "       ['65', '61', '58', ..., '56', '52', '44'],\n",
              "       ['50', '43', '54', ..., '49', '56', '47'],\n",
              "       ...,\n",
              "       ['91', '65', '42', ..., '72', '56', '43'],\n",
              "       ['77', '82', '79', ..., '105', '70', '46'],\n",
              "       ['77', '72', '84', ..., '106', '109', '82']], dtype='<U3')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_array= df.pixels.apply(lambda x: np.array(x.split(' ')).reshape(48,48,1).astype('float32'))"
      ],
      "metadata": {
        "id": "Y-SYI7xTzjIa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_array= np.stack(img_array,axis=0)"
      ],
      "metadata": {
        "id": "DIzphgfDz5kb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels= df.emotion.values"
      ],
      "metadata": {
        "id": "XuTAf19Tz_lO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test, y_train, y_test = train_test_split(img_array,labels,test_size=.2)"
      ],
      "metadata": {
        "id": "cuckhuh30VYr"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxVpQgnl1DGA",
        "outputId": "76228944-f650-4050-957a-2235070549aa"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7178, 48, 48, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hu2viAU1GsA",
        "outputId": "14d94ee6-5b55-4c5b-b71a-b5bd50d9dba6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7178,)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train= X_train/255\n",
        "X_test= X_test/255"
      ],
      "metadata": {
        "id": "Nw_FnI1y1Ik6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basemodel = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
        "    tf.keras.layers.MaxPool2D(2,2),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
        "    tf.keras.layers.MaxPool2D(2,2),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
        "    tf.keras.layers.MaxPool2D(2,2),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128,activation='relu'),\n",
        "    tf.keras.layers.Dense(7,activation='softmax')\n",
        "\n",
        "\n",
        "])\n"
      ],
      "metadata": {
        "id": "z2iFneOt1iph"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basemodel.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oe12YRs12bO",
        "outputId": "c0f186e5-636b-4c10-8168-b6d9f0f88e33"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 46, 46, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 23, 23, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 23, 23, 32)        128       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 21, 21, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 10, 10, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 10, 10, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 4, 4, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               262272    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 7)                 903       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 356743 (1.36 MB)\n",
            "Trainable params: 356295 (1.36 MB)\n",
            "Non-trainable params: 448 (1.75 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "basemodel.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.0001),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "mo7TOrT72JCy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "    os.mkdir('checkpoint')\n",
        "except:\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "zdcJYRRf3oux"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "file_name = 'best_model.h5'\n",
        "checkpoint_path = os.path.join('checkpoint', file_name)\n",
        "\n",
        "call_back = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                               monitor='val_accuracy',\n",
        "                                               verbose=1,\n",
        "                                               save_freq='epoch',\n",
        "                                               save_best_only=True,\n",
        "                                               save_weights_only=False,\n",
        "                                               mode='max')"
      ],
      "metadata": {
        "id": "2XqQzL1T367W"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basemodel.fit(X_train,y_train,epochs=20,validation_split=.1,callbacks=call_back)"
      ],
      "metadata": {
        "id": "9cfBdaJS5AP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, MaxPooling2D, Input, BatchNormalization, Dropout, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define your provided model architecture\n",
        "def create_model(input_shape=(48,48,1), num_classes=7):\n",
        "    input = Input(shape=input_shape)\n",
        "    x = Conv2D(filters=256, kernel_size=3, activation='relu', padding='same')(input)\n",
        "    x = Conv2D(filters=512, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2,2))(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "\n",
        "    x = Conv2D(filters=384, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2,2))(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "\n",
        "    x = Conv2D(filters=192, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2,2))(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "\n",
        "    x = Conv2D(filters=384, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2,2))(x)\n",
        "    x = Dropout(0.4)(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    return Model(input, x, name='fer_model')\n",
        "\n",
        "# Load your data and preprocess it\n",
        "# Assuming you have already defined X_train, y_train, X_test, y_test\n",
        "\n",
        "# Normalize pixel values\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "# Convert integer labels to one-hot encoded vectors\n",
        "y_train_encoded = to_categorical(y_train, num_classes=7)\n",
        "y_test_encoded = to_categorical(y_test, num_classes=7)\n",
        "\n",
        "# Create the model\n",
        "model = create_model()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Set up model checkpoint callback\n",
        "file_name = 'best_model.h5'\n",
        "checkpoint_path = os.path.join('checkpoint', file_name)\n",
        "call_back = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                               monitor='val_accuracy',\n",
        "                                               verbose=1,\n",
        "                                               save_freq='epoch',\n",
        "                                               save_best_only=True,\n",
        "                                               save_weights_only=False,\n",
        "                                               mode='max')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train_encoded, epochs=20, validation_split=0.2, callbacks=[call_back])\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test_encoded)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oumYCPOO6id4",
        "outputId": "aa500e35-ba9d-4199-f836-db630d36bfb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "718/718 [==============================] - ETA: 0s - loss: 1.9657 - accuracy: 0.2719\n",
            "Epoch 1: val_accuracy improved from -inf to 0.26228, saving model to checkpoint/best_model.h5\n",
            "718/718 [==============================] - 92s 114ms/step - loss: 1.9657 - accuracy: 0.2719 - val_loss: 1.9253 - val_accuracy: 0.2623\n",
            "Epoch 2/20\n",
            "718/718 [==============================] - ETA: 0s - loss: 1.5161 - accuracy: 0.4159\n",
            "Epoch 2: val_accuracy improved from 0.26228 to 0.37252, saving model to checkpoint/best_model.h5\n",
            "718/718 [==============================] - 77s 107ms/step - loss: 1.5161 - accuracy: 0.4159 - val_loss: 1.6472 - val_accuracy: 0.3725\n",
            "Epoch 3/20\n",
            "718/718 [==============================] - ETA: 0s - loss: 1.3164 - accuracy: 0.5004\n",
            "Epoch 3: val_accuracy improved from 0.37252 to 0.50644, saving model to checkpoint/best_model.h5\n",
            "718/718 [==============================] - 78s 109ms/step - loss: 1.3164 - accuracy: 0.5004 - val_loss: 1.3465 - val_accuracy: 0.5064\n",
            "Epoch 4/20\n",
            "718/718 [==============================] - ETA: 0s - loss: 1.2309 - accuracy: 0.5317\n",
            "Epoch 4: val_accuracy did not improve from 0.50644\n",
            "718/718 [==============================] - 77s 108ms/step - loss: 1.2309 - accuracy: 0.5317 - val_loss: 1.3795 - val_accuracy: 0.4619\n",
            "Epoch 5/20\n",
            "718/718 [==============================] - ETA: 0s - loss: 1.1596 - accuracy: 0.5612\n",
            "Epoch 5: val_accuracy improved from 0.50644 to 0.55051, saving model to checkpoint/best_model.h5\n",
            "718/718 [==============================] - 77s 108ms/step - loss: 1.1596 - accuracy: 0.5612 - val_loss: 1.1874 - val_accuracy: 0.5505\n",
            "Epoch 6/20\n",
            "718/718 [==============================] - ETA: 0s - loss: 1.1082 - accuracy: 0.5853\n",
            "Epoch 6: val_accuracy improved from 0.55051 to 0.56653, saving model to checkpoint/best_model.h5\n",
            "718/718 [==============================] - 78s 109ms/step - loss: 1.1082 - accuracy: 0.5853 - val_loss: 1.3399 - val_accuracy: 0.5665\n",
            "Epoch 7/20\n",
            "718/718 [==============================] - ETA: 0s - loss: 1.0610 - accuracy: 0.6036\n",
            "Epoch 7: val_accuracy improved from 0.56653 to 0.57349, saving model to checkpoint/best_model.h5\n",
            "718/718 [==============================] - 77s 108ms/step - loss: 1.0610 - accuracy: 0.6036 - val_loss: 1.1942 - val_accuracy: 0.5735\n",
            "Epoch 8/20\n",
            "718/718 [==============================] - ETA: 0s - loss: 1.0287 - accuracy: 0.6162\n",
            "Epoch 8: val_accuracy improved from 0.57349 to 0.58377, saving model to checkpoint/best_model.h5\n",
            "718/718 [==============================] - 77s 108ms/step - loss: 1.0287 - accuracy: 0.6162 - val_loss: 1.1726 - val_accuracy: 0.5838\n",
            "Epoch 9/20\n",
            "718/718 [==============================] - ETA: 0s - loss: 0.9895 - accuracy: 0.6301\n",
            "Epoch 9: val_accuracy did not improve from 0.58377\n",
            "718/718 [==============================] - 77s 108ms/step - loss: 0.9895 - accuracy: 0.6301 - val_loss: 1.5125 - val_accuracy: 0.5695\n",
            "Epoch 10/20\n",
            "718/718 [==============================] - ETA: 0s - loss: 0.9494 - accuracy: 0.6481\n",
            "Epoch 10: val_accuracy did not improve from 0.58377\n",
            "718/718 [==============================] - 77s 108ms/step - loss: 0.9494 - accuracy: 0.6481 - val_loss: 3.1581 - val_accuracy: 0.5225\n",
            "Epoch 11/20\n",
            "718/718 [==============================] - ETA: 0s - loss: 0.9216 - accuracy: 0.6592\n",
            "Epoch 11: val_accuracy improved from 0.58377 to 0.61041, saving model to checkpoint/best_model.h5\n",
            "718/718 [==============================] - 78s 109ms/step - loss: 0.9216 - accuracy: 0.6592 - val_loss: 1.2128 - val_accuracy: 0.6104\n",
            "Epoch 12/20\n",
            "718/718 [==============================] - ETA: 0s - loss: 0.8990 - accuracy: 0.6692\n",
            "Epoch 12: val_accuracy did not improve from 0.61041\n",
            "718/718 [==============================] - 78s 109ms/step - loss: 0.8990 - accuracy: 0.6692 - val_loss: 2.0766 - val_accuracy: 0.5627\n",
            "Epoch 13/20\n",
            "718/718 [==============================] - ETA: 0s - loss: 0.8632 - accuracy: 0.6788\n",
            "Epoch 13: val_accuracy did not improve from 0.61041\n",
            "718/718 [==============================] - 78s 109ms/step - loss: 0.8632 - accuracy: 0.6788 - val_loss: 1.6397 - val_accuracy: 0.5947\n",
            "Epoch 14/20\n",
            "718/718 [==============================] - ETA: 0s - loss: 0.8402 - accuracy: 0.6897\n",
            "Epoch 14: val_accuracy improved from 0.61041 to 0.61407, saving model to checkpoint/best_model.h5\n",
            "718/718 [==============================] - 78s 109ms/step - loss: 0.8402 - accuracy: 0.6897 - val_loss: 1.3179 - val_accuracy: 0.6141\n",
            "Epoch 15/20\n",
            "718/718 [==============================] - ETA: 0s - loss: 0.8058 - accuracy: 0.7025\n",
            "Epoch 15: val_accuracy did not improve from 0.61407\n",
            "718/718 [==============================] - 77s 108ms/step - loss: 0.8058 - accuracy: 0.7025 - val_loss: 1.4960 - val_accuracy: 0.6048\n",
            "Epoch 16/20\n",
            "718/718 [==============================] - ETA: 0s - loss: 0.7890 - accuracy: 0.7098\n",
            "Epoch 16: val_accuracy did not improve from 0.61407\n",
            "718/718 [==============================] - 78s 109ms/step - loss: 0.7890 - accuracy: 0.7098 - val_loss: 1.2576 - val_accuracy: 0.6083\n",
            "Epoch 17/20\n",
            "718/718 [==============================] - ETA: 0s - loss: 0.7666 - accuracy: 0.7170\n",
            "Epoch 17: val_accuracy did not improve from 0.61407\n",
            "718/718 [==============================] - 77s 108ms/step - loss: 0.7666 - accuracy: 0.7170 - val_loss: 2.2091 - val_accuracy: 0.5817\n",
            "Epoch 18/20\n",
            "622/718 [========================>.....] - ETA: 9s - loss: 0.7387 - accuracy: 0.7276"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mESYP4Fb7itw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}