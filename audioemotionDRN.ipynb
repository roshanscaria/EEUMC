{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 128, 1)]             0         []                            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 128, 32)              128       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1  (None, 64, 32)               0         ['conv1d[0][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 64, 64)               6208      ['max_pooling1d[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 64, 64)               256       ['conv1d_1[0][0]']            \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 64, 64)               0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 64, 64)               2112      ['max_pooling1d[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 64, 64)               12352     ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 64, 64)               256       ['conv1d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 64, 64)               256       ['conv1d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 64, 64)               0         ['batch_normalization_2[0][0]'\n",
      "                                                                    , 'batch_normalization_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 64, 64)               0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPoolin  (None, 32, 64)               0         ['activation_1[0][0]']        \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, 32, 128)              24704     ['max_pooling1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 32, 128)              512       ['conv1d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 32, 128)              0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 32, 128)              8320      ['max_pooling1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, 32, 128)              49280     ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 32, 128)              512       ['conv1d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 32, 128)              512       ['conv1d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 32, 128)              0         ['batch_normalization_5[0][0]'\n",
      "                                                                    , 'batch_normalization_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 32, 128)              0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPoolin  (None, 16, 128)              0         ['activation_3[0][0]']        \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 2048)                 0         ['max_pooling1d_2[0][0]']     \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 128)                  262272    ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 128)                  0         ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 14)                   1806      ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 369486 (1.41 MB)\n",
      "Trainable params: 368334 (1.41 MB)\n",
      "Non-trainable params: 1152 (4.50 KB)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "228/228 [==============================] - 4s 14ms/step - loss: 2.5012 - accuracy: 0.1557 - val_loss: 2.5344 - val_accuracy: 0.1496\n",
      "Epoch 2/50\n",
      "228/228 [==============================] - 3s 13ms/step - loss: 2.2978 - accuracy: 0.1946 - val_loss: 2.1739 - val_accuracy: 0.2712\n",
      "Epoch 3/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 2.1946 - accuracy: 0.2242 - val_loss: 2.0352 - val_accuracy: 0.2816\n",
      "Epoch 4/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 2.1161 - accuracy: 0.2533 - val_loss: 1.9375 - val_accuracy: 0.3436\n",
      "Epoch 5/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 2.0500 - accuracy: 0.2770 - val_loss: 1.8980 - val_accuracy: 0.3501\n",
      "Epoch 6/50\n",
      "228/228 [==============================] - 3s 13ms/step - loss: 2.0116 - accuracy: 0.2823 - val_loss: 1.8235 - val_accuracy: 0.3841\n",
      "Epoch 7/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.9613 - accuracy: 0.3052 - val_loss: 1.8102 - val_accuracy: 0.3918\n",
      "Epoch 8/50\n",
      "228/228 [==============================] - 3s 13ms/step - loss: 1.9208 - accuracy: 0.3214 - val_loss: 1.7328 - val_accuracy: 0.3967\n",
      "Epoch 9/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.8833 - accuracy: 0.3343 - val_loss: 1.7605 - val_accuracy: 0.4071\n",
      "Epoch 10/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.8430 - accuracy: 0.3432 - val_loss: 1.6942 - val_accuracy: 0.4142\n",
      "Epoch 11/50\n",
      "228/228 [==============================] - 3s 13ms/step - loss: 1.8176 - accuracy: 0.3535 - val_loss: 1.6904 - val_accuracy: 0.4230\n",
      "Epoch 12/50\n",
      "228/228 [==============================] - 4s 16ms/step - loss: 1.7669 - accuracy: 0.3583 - val_loss: 1.6599 - val_accuracy: 0.4121\n",
      "Epoch 13/50\n",
      "228/228 [==============================] - 4s 17ms/step - loss: 1.7631 - accuracy: 0.3692 - val_loss: 1.6514 - val_accuracy: 0.4296\n",
      "Epoch 14/50\n",
      "228/228 [==============================] - 3s 15ms/step - loss: 1.7373 - accuracy: 0.3725 - val_loss: 1.6881 - val_accuracy: 0.4153\n",
      "Epoch 15/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.7367 - accuracy: 0.3783 - val_loss: 1.6388 - val_accuracy: 0.4236\n",
      "Epoch 16/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.6810 - accuracy: 0.3875 - val_loss: 1.6240 - val_accuracy: 0.4351\n",
      "Epoch 17/50\n",
      "228/228 [==============================] - 3s 13ms/step - loss: 1.6755 - accuracy: 0.3914 - val_loss: 1.6824 - val_accuracy: 0.4126\n",
      "Epoch 18/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.6586 - accuracy: 0.3951 - val_loss: 1.6849 - val_accuracy: 0.4170\n",
      "Epoch 19/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.6221 - accuracy: 0.4075 - val_loss: 1.6383 - val_accuracy: 0.4301\n",
      "Epoch 20/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.6080 - accuracy: 0.4160 - val_loss: 1.5957 - val_accuracy: 0.4329\n",
      "Epoch 21/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.5984 - accuracy: 0.4110 - val_loss: 1.6033 - val_accuracy: 0.4312\n",
      "Epoch 22/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.5790 - accuracy: 0.4164 - val_loss: 1.5952 - val_accuracy: 0.4477\n",
      "Epoch 23/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.5462 - accuracy: 0.4249 - val_loss: 1.6207 - val_accuracy: 0.4301\n",
      "Epoch 24/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.5253 - accuracy: 0.4368 - val_loss: 1.6166 - val_accuracy: 0.4307\n",
      "Epoch 25/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.5095 - accuracy: 0.4333 - val_loss: 1.6182 - val_accuracy: 0.4493\n",
      "Epoch 26/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.4725 - accuracy: 0.4534 - val_loss: 1.5893 - val_accuracy: 0.4449\n",
      "Epoch 27/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.4704 - accuracy: 0.4519 - val_loss: 1.6250 - val_accuracy: 0.4455\n",
      "Epoch 28/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.4472 - accuracy: 0.4549 - val_loss: 1.6487 - val_accuracy: 0.4351\n",
      "Epoch 29/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.4238 - accuracy: 0.4560 - val_loss: 1.6005 - val_accuracy: 0.4559\n",
      "Epoch 30/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.4186 - accuracy: 0.4624 - val_loss: 1.6209 - val_accuracy: 0.4400\n",
      "Epoch 31/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.3856 - accuracy: 0.4763 - val_loss: 1.6057 - val_accuracy: 0.4411\n",
      "Epoch 32/50\n",
      "228/228 [==============================] - 3s 15ms/step - loss: 1.3785 - accuracy: 0.4735 - val_loss: 1.6965 - val_accuracy: 0.4274\n",
      "Epoch 33/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.3582 - accuracy: 0.4829 - val_loss: 1.6880 - val_accuracy: 0.4345\n",
      "Epoch 34/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.3337 - accuracy: 0.4904 - val_loss: 1.6569 - val_accuracy: 0.4422\n",
      "Epoch 35/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.3067 - accuracy: 0.4971 - val_loss: 1.7226 - val_accuracy: 0.4384\n",
      "Epoch 36/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.2910 - accuracy: 0.5071 - val_loss: 1.7479 - val_accuracy: 0.4274\n",
      "Epoch 37/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.2688 - accuracy: 0.5207 - val_loss: 1.7330 - val_accuracy: 0.4318\n",
      "Epoch 38/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.2449 - accuracy: 0.5306 - val_loss: 1.7556 - val_accuracy: 0.4345\n",
      "Epoch 39/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.2172 - accuracy: 0.5319 - val_loss: 1.7529 - val_accuracy: 0.4290\n",
      "Epoch 40/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.2135 - accuracy: 0.5281 - val_loss: 1.8102 - val_accuracy: 0.4334\n",
      "Epoch 41/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.1804 - accuracy: 0.5428 - val_loss: 1.8004 - val_accuracy: 0.4258\n",
      "Epoch 42/50\n",
      "228/228 [==============================] - 3s 13ms/step - loss: 1.1629 - accuracy: 0.5500 - val_loss: 1.8530 - val_accuracy: 0.4416\n",
      "Epoch 43/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.1557 - accuracy: 0.5572 - val_loss: 1.8536 - val_accuracy: 0.4279\n",
      "Epoch 44/50\n",
      "228/228 [==============================] - 3s 13ms/step - loss: 1.1429 - accuracy: 0.5595 - val_loss: 2.1019 - val_accuracy: 0.4192\n",
      "Epoch 45/50\n",
      "228/228 [==============================] - 3s 15ms/step - loss: 1.1242 - accuracy: 0.5647 - val_loss: 1.8717 - val_accuracy: 0.4312\n",
      "Epoch 46/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.1028 - accuracy: 0.5784 - val_loss: 1.9234 - val_accuracy: 0.4296\n",
      "Epoch 47/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.0819 - accuracy: 0.5807 - val_loss: 1.9623 - val_accuracy: 0.4405\n",
      "Epoch 48/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.0636 - accuracy: 0.5870 - val_loss: 1.9293 - val_accuracy: 0.4466\n",
      "Epoch 49/50\n",
      "228/228 [==============================] - 3s 14ms/step - loss: 1.0378 - accuracy: 0.5994 - val_loss: 2.0988 - val_accuracy: 0.4247\n",
      "Epoch 50/50\n",
      "228/228 [==============================] - 3s 15ms/step - loss: 1.0580 - accuracy: 0.5918 - val_loss: 1.9600 - val_accuracy: 0.4219\n",
      "96/96 [==============================] - 0s 4ms/step - loss: 1.9348 - accuracy: 0.4275\n",
      "Test accuracy: 0.4274909496307373\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift\n",
    "\n",
    "\n",
    "# Load the dataset and perform feature extraction\n",
    "df = pd.read_csv(\"/Users/roshanscaria/Desktop/Audio Emotion/Data_path.csv\")\n",
    "\n",
    "# Define augmentation pipeline\n",
    "augment = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.2, p=0.5),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5)\n",
    "])\n",
    "\n",
    "# Initialize lists to store features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Loop feature extraction over the entire dataset\n",
    "for label, path in zip(df['labels'], df['path']):\n",
    "    # Load audio file\n",
    "    X, sample_rate = librosa.load(path, res_type='kaiser_fast', duration=2.5, sr=44100, offset=0.5)\n",
    "\n",
    "    # Apply augmentation\n",
    "    X_augmented = augment(samples=X, sample_rate=sample_rate)\n",
    "\n",
    "    # Compute Mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=X_augmented, sr=sample_rate, n_mels=128)\n",
    "\n",
    "    # Convert to decibel scale (log-mel spectrogram)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Take the mean as the feature\n",
    "    feature = np.mean(log_mel_spectrogram, axis=1)\n",
    "\n",
    "    # Append feature and label to the lists\n",
    "    features.append(feature)\n",
    "    labels.append(label)\n",
    "\n",
    "# Convert features and labels to NumPy arrays\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Initialize a LabelEncoder object\n",
    "lb = LabelEncoder()\n",
    "\n",
    "# Encode the labels\n",
    "y_encoded = lb.fit_transform(y)\n",
    "\n",
    "# Perform one-hot encoding on the labels\n",
    "num_classes = len(np.unique(y))\n",
    "y_encoded = to_categorical(y_encoded, num_classes=num_classes)\n",
    "\n",
    "# Split between train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.25, stratify=y_encoded, random_state=42)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform both the training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the data to fit the input shape of the model\n",
    "X_train_reshaped = np.expand_dims(X_train_scaled, axis=2)\n",
    "X_test_reshaped = np.expand_dims(X_test_scaled, axis=2)\n",
    "\n",
    "# Define Residual Block\n",
    "def residual_block(x, filters, kernel_size, strides=1, activation='relu'):\n",
    "    y = layers.Conv1D(filters, kernel_size, strides=strides, padding='same')(x)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Activation(activation)(y)\n",
    "\n",
    "    y = layers.Conv1D(filters, kernel_size, padding='same')(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "\n",
    "    # Shortcut connection\n",
    "    if strides != 1 or x.shape[-1] != filters:\n",
    "        x = layers.Conv1D(filters, 1, strides=strides, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Merge\n",
    "    y = layers.add([x, y])\n",
    "    y = layers.Activation(activation)(y)\n",
    "    return y\n",
    "\n",
    "# Define VQ-MAE-S ResNet model architecture\n",
    "def build_resnet(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv1D(32, 3, activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = residual_block(x, filters=64, kernel_size=3)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = residual_block(x, filters=128, kernel_size=3)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "resnet_model = build_resnet(input_shape=X_train_reshaped.shape[1:], num_classes=num_classes)\n",
    "\n",
    "# Compile the model\n",
    "resnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "resnet_model.summary()\n",
    "\n",
    "# Train the model\n",
    "resnet_history = resnet_model.fit(X_train_reshaped, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = resnet_model.evaluate(X_test_reshaped, y_test)\n",
    "print(\"Test accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 128, 1)]             0         []                            \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, 128, 64)              256       ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPoolin  (None, 64, 64)               0         ['conv1d_7[0][0]']            \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)           (None, 64, 128)              24704     ['max_pooling1d_3[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 64, 128)              512       ['conv1d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 64, 128)              0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)          (None, 64, 128)              8320      ['max_pooling1d_3[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)           (None, 64, 128)              49280     ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 64, 128)              512       ['conv1d_10[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 64, 128)              512       ['conv1d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 64, 128)              0         ['batch_normalization_8[0][0]'\n",
      "                                                                    , 'batch_normalization_7[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 64, 128)              0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_4 (MaxPoolin  (None, 32, 128)              0         ['activation_5[0][0]']        \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)          (None, 32, 256)              98560     ['max_pooling1d_4[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 32, 256)              1024      ['conv1d_11[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)   (None, 32, 256)              0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_13 (Conv1D)          (None, 32, 256)              33024     ['max_pooling1d_4[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)          (None, 32, 256)              196864    ['activation_6[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 32, 256)              1024      ['conv1d_13[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 32, 256)              1024      ['conv1d_12[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 32, 256)              0         ['batch_normalization_11[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_7 (Activation)   (None, 32, 256)              0         ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_5 (MaxPoolin  (None, 16, 256)              0         ['activation_7[0][0]']        \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 4096)                 0         ['max_pooling1d_5[0][0]']     \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 256)                  1048832   ['flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 256)                  0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 14)                   3598      ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1468046 (5.60 MB)\n",
      "Trainable params: 1465742 (5.59 MB)\n",
      "Non-trainable params: 2304 (9.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "114/114 [==============================] - 6s 48ms/step - loss: 2.5727 - accuracy: 0.1564 - val_loss: 2.5866 - val_accuracy: 0.0921\n",
      "Epoch 2/100\n",
      "114/114 [==============================] - 5s 47ms/step - loss: 2.2478 - accuracy: 0.2264 - val_loss: 2.4790 - val_accuracy: 0.1452\n",
      "Epoch 3/100\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 2.1184 - accuracy: 0.2675 - val_loss: 2.2564 - val_accuracy: 0.2592\n",
      "Epoch 4/100\n",
      "114/114 [==============================] - 7s 62ms/step - loss: 2.0318 - accuracy: 0.2924 - val_loss: 2.0433 - val_accuracy: 0.3156\n",
      "Epoch 5/100\n",
      "114/114 [==============================] - 6s 54ms/step - loss: 1.9653 - accuracy: 0.3076 - val_loss: 1.8343 - val_accuracy: 0.3792\n",
      "Epoch 6/100\n",
      "114/114 [==============================] - 6s 55ms/step - loss: 1.9250 - accuracy: 0.3251 - val_loss: 1.7929 - val_accuracy: 0.3912\n",
      "Epoch 7/100\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 1.8666 - accuracy: 0.3361 - val_loss: 1.7394 - val_accuracy: 0.4148\n",
      "Epoch 8/100\n",
      "114/114 [==============================] - 6s 49ms/step - loss: 1.8502 - accuracy: 0.3391 - val_loss: 1.7710 - val_accuracy: 0.3858\n",
      "Epoch 9/100\n",
      "114/114 [==============================] - 6s 49ms/step - loss: 1.8186 - accuracy: 0.3502 - val_loss: 1.6878 - val_accuracy: 0.4115\n",
      "Epoch 10/100\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 1.7771 - accuracy: 0.3568 - val_loss: 1.6808 - val_accuracy: 0.4197\n",
      "Epoch 11/100\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 1.7436 - accuracy: 0.3745 - val_loss: 1.6615 - val_accuracy: 0.4077\n",
      "Epoch 12/100\n",
      "114/114 [==============================] - 6s 50ms/step - loss: 1.7275 - accuracy: 0.3812 - val_loss: 1.6718 - val_accuracy: 0.4181\n",
      "Epoch 13/100\n",
      "114/114 [==============================] - 6s 49ms/step - loss: 1.7083 - accuracy: 0.3750 - val_loss: 1.7173 - val_accuracy: 0.3984\n",
      "Epoch 14/100\n",
      "114/114 [==============================] - 6s 49ms/step - loss: 1.6805 - accuracy: 0.3895 - val_loss: 1.6572 - val_accuracy: 0.4208\n",
      "Epoch 15/100\n",
      "114/114 [==============================] - 6s 49ms/step - loss: 1.6522 - accuracy: 0.4009 - val_loss: 1.6472 - val_accuracy: 0.4274\n",
      "Epoch 16/100\n",
      "114/114 [==============================] - 7s 63ms/step - loss: 1.6324 - accuracy: 0.4021 - val_loss: 1.6590 - val_accuracy: 0.4241\n",
      "Epoch 17/100\n",
      "114/114 [==============================] - 6s 55ms/step - loss: 1.5960 - accuracy: 0.4215 - val_loss: 1.6405 - val_accuracy: 0.4208\n",
      "Epoch 18/100\n",
      "114/114 [==============================] - 7s 59ms/step - loss: 1.5910 - accuracy: 0.4104 - val_loss: 1.6317 - val_accuracy: 0.4192\n",
      "Epoch 19/100\n",
      "114/114 [==============================] - 7s 58ms/step - loss: 1.5689 - accuracy: 0.4206 - val_loss: 1.6465 - val_accuracy: 0.4241\n",
      "Epoch 20/100\n",
      "114/114 [==============================] - 6s 52ms/step - loss: 1.5329 - accuracy: 0.4279 - val_loss: 1.6415 - val_accuracy: 0.4285\n",
      "Epoch 21/100\n",
      "114/114 [==============================] - 6s 53ms/step - loss: 1.5211 - accuracy: 0.4296 - val_loss: 1.6375 - val_accuracy: 0.4416\n",
      "Epoch 22/100\n",
      "114/114 [==============================] - 6s 54ms/step - loss: 1.5000 - accuracy: 0.4385 - val_loss: 1.6268 - val_accuracy: 0.4378\n",
      "Epoch 23/100\n",
      "114/114 [==============================] - 6s 54ms/step - loss: 1.4677 - accuracy: 0.4531 - val_loss: 1.6709 - val_accuracy: 0.4312\n",
      "Epoch 24/100\n",
      "114/114 [==============================] - 6s 53ms/step - loss: 1.4416 - accuracy: 0.4589 - val_loss: 1.6766 - val_accuracy: 0.4290\n",
      "Epoch 25/100\n",
      "114/114 [==============================] - 6s 53ms/step - loss: 1.4240 - accuracy: 0.4626 - val_loss: 1.6519 - val_accuracy: 0.4329\n",
      "Epoch 26/100\n",
      "114/114 [==============================] - 6s 55ms/step - loss: 1.3957 - accuracy: 0.4774 - val_loss: 1.6818 - val_accuracy: 0.4422\n",
      "Epoch 27/100\n",
      "114/114 [==============================] - 6s 54ms/step - loss: 1.3705 - accuracy: 0.4759 - val_loss: 1.6908 - val_accuracy: 0.4405\n",
      "96/96 [==============================] - 1s 8ms/step - loss: 1.6943 - accuracy: 0.4157\n",
      "Test accuracy: 0.4156527519226074\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the dataset and perform feature extraction\n",
    "df = pd.read_csv(\"/Users/roshanscaria/Desktop/Audio Emotion/Data_path.csv\")\n",
    "\n",
    "# Define augmentation pipeline\n",
    "augment = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.02, p=0.5),\n",
    "    TimeStretch(min_rate=0.7, max_rate=1.3, p=0.5),\n",
    "    PitchShift(min_semitones=-5, max_semitones=5, p=0.5)\n",
    "])\n",
    "\n",
    "# Initialize lists to store features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Loop feature extraction over the entire dataset\n",
    "for label, path in zip(df['labels'], df['path']):\n",
    "    # Load audio file\n",
    "    X, sample_rate = librosa.load(path, res_type='kaiser_fast', duration=2.5, sr=44100, offset=0.5)\n",
    "\n",
    "    # Apply augmentation\n",
    "    X_augmented = augment(samples=X, sample_rate=sample_rate)\n",
    "\n",
    "    # Compute Mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=X_augmented, sr=sample_rate, n_mels=128)\n",
    "\n",
    "    # Convert to decibel scale (log-mel spectrogram)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Take the mean as the feature\n",
    "    feature = np.mean(log_mel_spectrogram, axis=1)\n",
    "\n",
    "    # Append feature and label to the lists\n",
    "    features.append(feature)\n",
    "    labels.append(label)\n",
    "\n",
    "# Convert features and labels to NumPy arrays\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Initialize a LabelEncoder object\n",
    "lb = LabelEncoder()\n",
    "\n",
    "# Encode the labels\n",
    "y_encoded = lb.fit_transform(y)\n",
    "\n",
    "# Perform one-hot encoding on the labels\n",
    "num_classes = len(np.unique(y))\n",
    "y_encoded = to_categorical(y_encoded, num_classes=num_classes)\n",
    "\n",
    "# Split between train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.25, stratify=y_encoded, random_state=42)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform both the training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the data to fit the input shape of the model\n",
    "X_train_reshaped = np.expand_dims(X_train_scaled, axis=2)\n",
    "X_test_reshaped = np.expand_dims(X_test_scaled, axis=2)\n",
    "\n",
    "# Define Residual Block\n",
    "def residual_block(x, filters, kernel_size, strides=1, activation='relu'):\n",
    "    y = layers.Conv1D(filters, kernel_size, strides=strides, padding='same')(x)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Activation(activation)(y)\n",
    "\n",
    "    y = layers.Conv1D(filters, kernel_size, padding='same')(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "\n",
    "    # Shortcut connection\n",
    "    if strides != 1 or x.shape[-1] != filters:\n",
    "        x = layers.Conv1D(filters, 1, strides=strides, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Merge\n",
    "    y = layers.add([x, y])\n",
    "    y = layers.Activation(activation)(y)\n",
    "    return y\n",
    "\n",
    "# Define VQ-MAE-S ResNet model architecture\n",
    "def build_resnet(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = residual_block(x, filters=128, kernel_size=3)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = residual_block(x, filters=256, kernel_size=3)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "resnet_model = build_resnet(input_shape=X_train_reshaped.shape[1:], num_classes=num_classes)\n",
    "\n",
    "# Adjust learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Compile the model\n",
    "resnet_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "resnet_model.summary()\n",
    "\n",
    "# Train the model\n",
    "resnet_history = resnet_model.fit(X_train_reshaped, y_train, epochs=100, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = resnet_model.evaluate(X_test_reshaped, y_test)\n",
    "print(\"Test accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)        [(None, 128, 1)]             0         []                            \n",
      "                                                                                                  \n",
      " conv1d_14 (Conv1D)          (None, 128, 64)              256       ['input_3[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling1d_6 (MaxPoolin  (None, 64, 64)               0         ['conv1d_14[0][0]']           \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " conv1d_15 (Conv1D)          (None, 64, 128)              24704     ['max_pooling1d_6[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, 64, 128)              512       ['conv1d_15[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_8 (Activation)   (None, 64, 128)              0         ['batch_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_17 (Conv1D)          (None, 64, 128)              8320      ['max_pooling1d_6[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d_16 (Conv1D)          (None, 64, 128)              49280     ['activation_8[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 64, 128)              512       ['conv1d_17[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 64, 128)              512       ['conv1d_16[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 64, 128)              0         ['batch_normalization_14[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_9 (Activation)   (None, 64, 128)              0         ['add_4[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_7 (MaxPoolin  (None, 32, 128)              0         ['activation_9[0][0]']        \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " conv1d_18 (Conv1D)          (None, 32, 256)              98560     ['max_pooling1d_7[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 32, 256)              1024      ['conv1d_18[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_10 (Activation)  (None, 32, 256)              0         ['batch_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_20 (Conv1D)          (None, 32, 256)              33024     ['max_pooling1d_7[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d_19 (Conv1D)          (None, 32, 256)              196864    ['activation_10[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_17 (Ba  (None, 32, 256)              1024      ['conv1d_20[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, 32, 256)              1024      ['conv1d_19[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 32, 256)              0         ['batch_normalization_17[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_11 (Activation)  (None, 32, 256)              0         ['add_5[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_8 (MaxPoolin  (None, 16, 256)              0         ['activation_11[0][0]']       \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " conv1d_21 (Conv1D)          (None, 16, 512)              393728    ['max_pooling1d_8[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_18 (Ba  (None, 16, 512)              2048      ['conv1d_21[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_12 (Activation)  (None, 16, 512)              0         ['batch_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv1d_23 (Conv1D)          (None, 16, 512)              131584    ['max_pooling1d_8[0][0]']     \n",
      "                                                                                                  \n",
      " conv1d_22 (Conv1D)          (None, 16, 512)              786944    ['activation_12[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_20 (Ba  (None, 16, 512)              2048      ['conv1d_23[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 16, 512)              2048      ['conv1d_22[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 16, 512)              0         ['batch_normalization_20[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_19[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_13 (Activation)  (None, 16, 512)              0         ['add_6[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling1d_9 (MaxPoolin  (None, 8, 512)               0         ['activation_13[0][0]']       \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)         (None, 4096)                 0         ['max_pooling1d_9[0][0]']     \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 512)                  2097664   ['flatten_2[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 512)                  0         ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 14)                   7182      ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3838862 (14.64 MB)\n",
      "Trainable params: 3833486 (14.62 MB)\n",
      "Non-trainable params: 5376 (21.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/100\n",
      "114/114 [==============================] - 17s 130ms/step - loss: 2.5317 - accuracy: 0.1963 - val_loss: 2.6309 - val_accuracy: 0.0674 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "114/114 [==============================] - 14s 124ms/step - loss: 2.1065 - accuracy: 0.2911 - val_loss: 2.6386 - val_accuracy: 0.0805 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "114/114 [==============================] - 14s 126ms/step - loss: 1.9440 - accuracy: 0.3447 - val_loss: 2.5172 - val_accuracy: 0.1408 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "114/114 [==============================] - 14s 123ms/step - loss: 1.8490 - accuracy: 0.3673 - val_loss: 2.2470 - val_accuracy: 0.2351 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "114/114 [==============================] - 15s 128ms/step - loss: 1.7293 - accuracy: 0.4157 - val_loss: 1.9122 - val_accuracy: 0.3468 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "114/114 [==============================] - 15s 128ms/step - loss: 1.6379 - accuracy: 0.4322 - val_loss: 1.7995 - val_accuracy: 0.3715 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "114/114 [==============================] - 15s 128ms/step - loss: 1.5640 - accuracy: 0.4635 - val_loss: 1.7198 - val_accuracy: 0.4071 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "114/114 [==============================] - 14s 126ms/step - loss: 1.4719 - accuracy: 0.4905 - val_loss: 1.7119 - val_accuracy: 0.4082 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "114/114 [==============================] - 15s 129ms/step - loss: 1.3952 - accuracy: 0.5162 - val_loss: 1.6714 - val_accuracy: 0.4142 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "114/114 [==============================] - 14s 125ms/step - loss: 1.3130 - accuracy: 0.5444 - val_loss: 1.6736 - val_accuracy: 0.4214 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "114/114 [==============================] - 15s 133ms/step - loss: 1.2449 - accuracy: 0.5700 - val_loss: 1.6565 - val_accuracy: 0.4268 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "114/114 [==============================] - 15s 134ms/step - loss: 1.1751 - accuracy: 0.6014 - val_loss: 1.7220 - val_accuracy: 0.4077 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "114/114 [==============================] - 15s 131ms/step - loss: 1.1231 - accuracy: 0.6039 - val_loss: 1.6600 - val_accuracy: 0.4384 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "114/114 [==============================] - 14s 126ms/step - loss: 1.0274 - accuracy: 0.6446 - val_loss: 1.6919 - val_accuracy: 0.4351 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "114/114 [==============================] - 14s 125ms/step - loss: 0.8654 - accuracy: 0.7075 - val_loss: 1.6802 - val_accuracy: 0.4553 - lr: 5.0000e-05\n",
      "Epoch 16/100\n",
      "114/114 [==============================] - 14s 124ms/step - loss: 0.7869 - accuracy: 0.7473 - val_loss: 1.6918 - val_accuracy: 0.4395 - lr: 5.0000e-05\n",
      "96/96 [==============================] - 2s 16ms/step - loss: 1.6391 - accuracy: 0.4341\n",
      "Test accuracy: 0.4340677261352539\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the dataset and perform feature extraction\n",
    "df = pd.read_csv(\"/Users/roshanscaria/Desktop/Audio Emotion/Data_path.csv\")\n",
    "\n",
    "# Define augmentation pipeline\n",
    "augment = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.02, p=0.5),\n",
    "    TimeStretch(min_rate=0.7, max_rate=1.3, p=0.5),\n",
    "    PitchShift(min_semitones=-5, max_semitones=5, p=0.5)\n",
    "])\n",
    "\n",
    "# Initialize lists to store features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Loop feature extraction over the entire dataset\n",
    "for label, path in zip(df['labels'], df['path']):\n",
    "    # Load audio file\n",
    "    X, sample_rate = librosa.load(path, res_type='kaiser_fast', duration=2.5, sr=44100, offset=0.5)\n",
    "\n",
    "    # Apply augmentation\n",
    "    X_augmented = augment(samples=X, sample_rate=sample_rate)\n",
    "\n",
    "    # Compute Mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=X_augmented, sr=sample_rate, n_mels=128)\n",
    "\n",
    "    # Convert to decibel scale (log-mel spectrogram)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Take the mean as the feature\n",
    "    feature = np.mean(log_mel_spectrogram, axis=1)\n",
    "\n",
    "    # Append feature and label to the lists\n",
    "    features.append(feature)\n",
    "    labels.append(label)\n",
    "\n",
    "# Convert features and labels to NumPy arrays\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Initialize a LabelEncoder object\n",
    "lb = LabelEncoder()\n",
    "\n",
    "# Encode the labels\n",
    "y_encoded = lb.fit_transform(y)\n",
    "\n",
    "# Perform one-hot encoding on the labels\n",
    "num_classes = len(np.unique(y))\n",
    "y_encoded = to_categorical(y_encoded, num_classes=num_classes)\n",
    "\n",
    "# Split between train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.25, stratify=y_encoded, random_state=42)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform both the training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the data to fit the input shape of the model\n",
    "X_train_reshaped = np.expand_dims(X_train_scaled, axis=2)\n",
    "X_test_reshaped = np.expand_dims(X_test_scaled, axis=2)\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Define Residual Block\n",
    "def residual_block(x, filters, kernel_size, strides=1, activation='relu'):\n",
    "    y = layers.Conv1D(filters, kernel_size, strides=strides, padding='same')(x)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Activation(activation)(y)\n",
    "\n",
    "    y = layers.Conv1D(filters, kernel_size, padding='same')(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "\n",
    "    # Shortcut connection\n",
    "    if strides != 1 or x.shape[-1] != filters:\n",
    "        x = layers.Conv1D(filters, 1, strides=strides, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Merge\n",
    "    y = layers.add([x, y])\n",
    "    y = layers.Activation(activation)(y)\n",
    "    return y\n",
    "\n",
    "# Define VQ-MAE-S ResNet model architecture with increased complexity\n",
    "def build_resnet(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = residual_block(x, filters=128, kernel_size=3)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = residual_block(x, filters=256, kernel_size=3)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = residual_block(x, filters=512, kernel_size=3)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "resnet_model = build_resnet(input_shape=X_train_reshaped.shape[1:], num_classes=num_classes)\n",
    "\n",
    "# Compile the model with the legacy optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "resnet_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "resnet_model.summary()\n",
    "\n",
    "# Train the model with the learning rate scheduler and early stopping\n",
    "resnet_history = resnet_model.fit(X_train_reshaped, y_train, epochs=100, batch_size=64, validation_split=0.2, callbacks=[lr_scheduler, early_stopping])\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = resnet_model.evaluate(X_test_reshaped, y_test)\n",
    "print(\"Test accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 128, 1)]          0         \n",
      "                                                                 \n",
      " conv1d_24 (Conv1D)          (None, 128, 64)           256       \n",
      "                                                                 \n",
      " max_pooling1d_10 (MaxPooli  (None, 64, 64)            0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_25 (Conv1D)          (None, 64, 128)           24704     \n",
      "                                                                 \n",
      " max_pooling1d_11 (MaxPooli  (None, 32, 128)           0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_26 (Conv1D)          (None, 32, 256)           98560     \n",
      "                                                                 \n",
      " max_pooling1d_12 (MaxPooli  (None, 16, 256)           0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_27 (Conv1D)          (None, 16, 512)           393728    \n",
      "                                                                 \n",
      " max_pooling1d_13 (MaxPooli  (None, 8, 512)            0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               2097664   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 14)                7182      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2622094 (10.00 MB)\n",
      "Trainable params: 2622094 (10.00 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "228/228 [==============================] - 5s 20ms/step - loss: 2.2920 - accuracy: 0.1997 - val_loss: 2.0096 - val_accuracy: 0.3200\n",
      "Epoch 2/50\n",
      "228/228 [==============================] - 4s 19ms/step - loss: 1.9256 - accuracy: 0.3425 - val_loss: 1.7807 - val_accuracy: 0.3814\n",
      "Epoch 3/50\n",
      "228/228 [==============================] - 5s 22ms/step - loss: 1.7165 - accuracy: 0.4046 - val_loss: 1.6489 - val_accuracy: 0.4093\n",
      "Epoch 4/50\n",
      "228/228 [==============================] - 6s 24ms/step - loss: 1.6003 - accuracy: 0.4405 - val_loss: 1.5997 - val_accuracy: 0.4334\n",
      "Epoch 5/50\n",
      "228/228 [==============================] - 6s 24ms/step - loss: 1.4810 - accuracy: 0.4800 - val_loss: 1.5418 - val_accuracy: 0.4433\n",
      "Epoch 6/50\n",
      "228/228 [==============================] - 5s 22ms/step - loss: 1.4108 - accuracy: 0.4989 - val_loss: 1.5245 - val_accuracy: 0.4718\n",
      "Epoch 7/50\n",
      "228/228 [==============================] - 5s 22ms/step - loss: 1.3258 - accuracy: 0.5247 - val_loss: 1.4639 - val_accuracy: 0.4745\n",
      "Epoch 8/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 1.2424 - accuracy: 0.5532 - val_loss: 1.5025 - val_accuracy: 0.4822\n",
      "Epoch 9/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 1.1549 - accuracy: 0.5870 - val_loss: 1.4794 - val_accuracy: 0.4773\n",
      "Epoch 10/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 1.0673 - accuracy: 0.6180 - val_loss: 1.5245 - val_accuracy: 0.4756\n",
      "Epoch 11/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.9848 - accuracy: 0.6451 - val_loss: 1.6165 - val_accuracy: 0.4658\n",
      "Epoch 12/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.8836 - accuracy: 0.6802 - val_loss: 1.6672 - val_accuracy: 0.4745\n",
      "Epoch 13/50\n",
      "228/228 [==============================] - 5s 22ms/step - loss: 0.7983 - accuracy: 0.7126 - val_loss: 1.7329 - val_accuracy: 0.4701\n",
      "Epoch 14/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.7049 - accuracy: 0.7458 - val_loss: 1.8583 - val_accuracy: 0.4658\n",
      "Epoch 15/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.6461 - accuracy: 0.7745 - val_loss: 1.9921 - val_accuracy: 0.4625\n",
      "Epoch 16/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.5473 - accuracy: 0.8011 - val_loss: 2.0966 - val_accuracy: 0.4647\n",
      "Epoch 17/50\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.4892 - accuracy: 0.8222 - val_loss: 2.1945 - val_accuracy: 0.4641\n",
      "Epoch 18/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.4681 - accuracy: 0.8320 - val_loss: 2.3110 - val_accuracy: 0.4559\n",
      "Epoch 19/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.4000 - accuracy: 0.8543 - val_loss: 2.4512 - val_accuracy: 0.4619\n",
      "Epoch 20/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.3529 - accuracy: 0.8735 - val_loss: 2.4607 - val_accuracy: 0.4597\n",
      "Epoch 21/50\n",
      "228/228 [==============================] - 6s 25ms/step - loss: 0.3232 - accuracy: 0.8857 - val_loss: 2.6832 - val_accuracy: 0.4559\n",
      "Epoch 22/50\n",
      "228/228 [==============================] - 5s 22ms/step - loss: 0.2988 - accuracy: 0.8932 - val_loss: 2.8238 - val_accuracy: 0.4619\n",
      "Epoch 23/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.2972 - accuracy: 0.8969 - val_loss: 2.9751 - val_accuracy: 0.4575\n",
      "Epoch 24/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.2759 - accuracy: 0.9060 - val_loss: 2.9275 - val_accuracy: 0.4438\n",
      "Epoch 25/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.2464 - accuracy: 0.9112 - val_loss: 3.0750 - val_accuracy: 0.4685\n",
      "Epoch 26/50\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.2372 - accuracy: 0.9160 - val_loss: 3.1005 - val_accuracy: 0.4603\n",
      "Epoch 27/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.2016 - accuracy: 0.9296 - val_loss: 3.4058 - val_accuracy: 0.4564\n",
      "Epoch 28/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.1787 - accuracy: 0.9374 - val_loss: 3.3381 - val_accuracy: 0.4559\n",
      "Epoch 29/50\n",
      "228/228 [==============================] - 5s 22ms/step - loss: 0.2202 - accuracy: 0.9256 - val_loss: 3.3302 - val_accuracy: 0.4422\n",
      "Epoch 30/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.1961 - accuracy: 0.9335 - val_loss: 3.4926 - val_accuracy: 0.4460\n",
      "Epoch 31/50\n",
      "228/228 [==============================] - 5s 22ms/step - loss: 0.1740 - accuracy: 0.9407 - val_loss: 3.5173 - val_accuracy: 0.4521\n",
      "Epoch 32/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.1480 - accuracy: 0.9465 - val_loss: 3.6559 - val_accuracy: 0.4477\n",
      "Epoch 33/50\n",
      "228/228 [==============================] - 5s 22ms/step - loss: 0.1699 - accuracy: 0.9431 - val_loss: 3.8241 - val_accuracy: 0.4367\n",
      "Epoch 34/50\n",
      "228/228 [==============================] - 5s 22ms/step - loss: 0.1482 - accuracy: 0.9487 - val_loss: 3.9690 - val_accuracy: 0.4575\n",
      "Epoch 35/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.1607 - accuracy: 0.9471 - val_loss: 4.0533 - val_accuracy: 0.4526\n",
      "Epoch 36/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.1526 - accuracy: 0.9492 - val_loss: 3.9047 - val_accuracy: 0.4493\n",
      "Epoch 37/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.1280 - accuracy: 0.9555 - val_loss: 3.9887 - val_accuracy: 0.4493\n",
      "Epoch 38/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.1333 - accuracy: 0.9519 - val_loss: 4.1611 - val_accuracy: 0.4608\n",
      "Epoch 39/50\n",
      "228/228 [==============================] - 5s 24ms/step - loss: 0.1864 - accuracy: 0.9416 - val_loss: 3.8441 - val_accuracy: 0.4537\n",
      "Epoch 40/50\n",
      "228/228 [==============================] - 5s 22ms/step - loss: 0.1098 - accuracy: 0.9630 - val_loss: 4.2043 - val_accuracy: 0.4537\n",
      "Epoch 41/50\n",
      "228/228 [==============================] - 5s 23ms/step - loss: 0.1187 - accuracy: 0.9590 - val_loss: 4.1915 - val_accuracy: 0.4537\n",
      "Epoch 42/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.1469 - accuracy: 0.9522 - val_loss: 4.2797 - val_accuracy: 0.4542\n",
      "Epoch 43/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.1392 - accuracy: 0.9545 - val_loss: 3.9572 - val_accuracy: 0.4619\n",
      "Epoch 44/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.1244 - accuracy: 0.9601 - val_loss: 4.0433 - val_accuracy: 0.4477\n",
      "Epoch 45/50\n",
      "228/228 [==============================] - 5s 22ms/step - loss: 0.0826 - accuracy: 0.9726 - val_loss: 4.4514 - val_accuracy: 0.4559\n",
      "Epoch 46/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.1032 - accuracy: 0.9656 - val_loss: 4.6864 - val_accuracy: 0.4477\n",
      "Epoch 47/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.1214 - accuracy: 0.9613 - val_loss: 4.2372 - val_accuracy: 0.4504\n",
      "Epoch 48/50\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.1016 - accuracy: 0.9648 - val_loss: 4.3824 - val_accuracy: 0.4510\n",
      "Epoch 49/50\n",
      "228/228 [==============================] - 5s 22ms/step - loss: 0.1225 - accuracy: 0.9601 - val_loss: 4.6794 - val_accuracy: 0.4466\n",
      "Epoch 50/50\n",
      "228/228 [==============================] - 5s 22ms/step - loss: 0.1148 - accuracy: 0.9627 - val_loss: 4.5063 - val_accuracy: 0.4493\n",
      "96/96 [==============================] - 1s 6ms/step - loss: 4.5538 - accuracy: 0.4390\n",
      "Test accuracy: 0.43900033831596375\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the dataset and perform feature extraction\n",
    "df = pd.read_csv(\"/Users/roshanscaria/Desktop/Audio Emotion/Data_path.csv\")\n",
    "\n",
    "# Define augmentation pipeline\n",
    "augment = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.02, p=0.5),\n",
    "    TimeStretch(min_rate=0.7, max_rate=1.3, p=0.5),\n",
    "    PitchShift(min_semitones=-5, max_semitones=5, p=0.5)\n",
    "])\n",
    "\n",
    "# Initialize lists to store features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Loop feature extraction over the entire dataset\n",
    "for label, path in zip(df['labels'], df['path']):\n",
    "    # Load audio file\n",
    "    X, sample_rate = librosa.load(path, res_type='kaiser_fast', duration=2.5, sr=44100, offset=0.5)\n",
    "\n",
    "    # Apply augmentation\n",
    "    X_augmented = augment(samples=X, sample_rate=sample_rate)\n",
    "\n",
    "    # Compute Mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=X_augmented, sr=sample_rate, n_mels=128)\n",
    "\n",
    "    # Convert to decibel scale (log-mel spectrogram)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Take the mean as the feature\n",
    "    feature = np.mean(log_mel_spectrogram, axis=1)\n",
    "\n",
    "    # Append feature and label to the lists\n",
    "    features.append(feature)\n",
    "    labels.append(label)\n",
    "\n",
    "# Convert features and labels to NumPy arrays\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Initialize a LabelEncoder object\n",
    "lb = LabelEncoder()\n",
    "\n",
    "# Encode the labels\n",
    "y_encoded = lb.fit_transform(y)\n",
    "\n",
    "# Perform one-hot encoding on the labels\n",
    "num_classes = len(np.unique(y))\n",
    "y_encoded = to_categorical(y_encoded, num_classes=num_classes)\n",
    "\n",
    "# Split between train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.25, stratify=y_encoded, random_state=42)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform both the training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the data to fit the input shape of the model\n",
    "X_train_reshaped = np.expand_dims(X_train_scaled, axis=2)\n",
    "X_test_reshaped = np.expand_dims(X_test_scaled, axis=2)\n",
    "\n",
    "# Define VQ-MAE-S ResNet model architecture with increased complexity\n",
    "def build_resnet(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.Conv1D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.Conv1D(256, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.Conv1D(512, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "resnet_model = build_resnet(input_shape=X_train_reshaped.shape[1:], num_classes=num_classes)\n",
    "\n",
    "# Compile the model\n",
    "resnet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "resnet_model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = resnet_model.fit(X_train_reshaped, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = resnet_model.evaluate(X_test_reshaped, y_test)\n",
    "print(\"Test accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 128, 1)]          0         \n",
      "                                                                 \n",
      " conv1d_28 (Conv1D)          (None, 128, 64)           256       \n",
      "                                                                 \n",
      " max_pooling1d_14 (MaxPooli  (None, 64, 64)            0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_29 (Conv1D)          (None, 64, 128)           24704     \n",
      "                                                                 \n",
      " max_pooling1d_15 (MaxPooli  (None, 32, 128)           0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_30 (Conv1D)          (None, 32, 256)           98560     \n",
      "                                                                 \n",
      " max_pooling1d_16 (MaxPooli  (None, 16, 256)           0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_31 (Conv1D)          (None, 16, 512)           393728    \n",
      "                                                                 \n",
      " max_pooling1d_17 (MaxPooli  (None, 8, 512)            0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 512)               2097664   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 14)                7182      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2622094 (10.00 MB)\n",
      "Trainable params: 2622094 (10.00 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 2.3235 - accuracy: 0.1961 - val_loss: 2.1100 - val_accuracy: 0.2904\n",
      "Epoch 2/100\n",
      "228/228 [==============================] - 5s 22ms/step - loss: 1.9600 - accuracy: 0.3298 - val_loss: 1.8145 - val_accuracy: 0.3671\n",
      "Epoch 3/100\n",
      "228/228 [==============================] - 5s 22ms/step - loss: 1.7377 - accuracy: 0.3902 - val_loss: 1.7164 - val_accuracy: 0.3737\n",
      "Epoch 4/100\n",
      "228/228 [==============================] - 5s 20ms/step - loss: 1.6009 - accuracy: 0.4360 - val_loss: 1.6064 - val_accuracy: 0.4088\n",
      "Epoch 5/100\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 1.5004 - accuracy: 0.4722 - val_loss: 1.5491 - val_accuracy: 0.4312\n",
      "Epoch 6/100\n",
      "228/228 [==============================] - 5s 20ms/step - loss: 1.4126 - accuracy: 0.5001 - val_loss: 1.5445 - val_accuracy: 0.4378\n",
      "Epoch 7/100\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 1.3335 - accuracy: 0.5223 - val_loss: 1.5476 - val_accuracy: 0.4510\n",
      "Epoch 8/100\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 1.2519 - accuracy: 0.5518 - val_loss: 1.5345 - val_accuracy: 0.4521\n",
      "Epoch 9/100\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 1.1669 - accuracy: 0.5744 - val_loss: 1.5610 - val_accuracy: 0.4504\n",
      "Epoch 10/100\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 1.0810 - accuracy: 0.6139 - val_loss: 1.5998 - val_accuracy: 0.4466\n",
      "Epoch 11/100\n",
      "228/228 [==============================] - 5s 22ms/step - loss: 0.9992 - accuracy: 0.6383 - val_loss: 1.6535 - val_accuracy: 0.4641\n",
      "Epoch 12/100\n",
      "228/228 [==============================] - 5s 22ms/step - loss: 0.9110 - accuracy: 0.6761 - val_loss: 1.6857 - val_accuracy: 0.4521\n",
      "Epoch 13/100\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.8189 - accuracy: 0.7053 - val_loss: 1.7113 - val_accuracy: 0.4597\n",
      "96/96 [==============================] - 1s 7ms/step - loss: 1.5693 - accuracy: 0.4594\n",
      "Test accuracy: 0.4593883454799652\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the dataset and perform feature extraction\n",
    "df = pd.read_csv(\"/Users/roshanscaria/Desktop/Audio Emotion/Data_path.csv\")\n",
    "\n",
    "# Define augmentation pipeline\n",
    "augment = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.02, p=0.5),\n",
    "    TimeStretch(min_rate=0.7, max_rate=1.3, p=0.5),\n",
    "    PitchShift(min_semitones=-5, max_semitones=5, p=0.5)\n",
    "])\n",
    "\n",
    "# Initialize lists to store features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Loop feature extraction over the entire dataset\n",
    "for label, path in zip(df['labels'], df['path']):\n",
    "    # Load audio file\n",
    "    X, sample_rate = librosa.load(path, res_type='kaiser_fast', duration=2.5, sr=44100, offset=0.5)\n",
    "\n",
    "    # Apply augmentation\n",
    "    X_augmented = augment(samples=X, sample_rate=sample_rate)\n",
    "\n",
    "    # Compute Mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=X_augmented, sr=sample_rate, n_mels=128)\n",
    "\n",
    "    # Convert to decibel scale (log-mel spectrogram)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Take the mean as the feature\n",
    "    feature = np.mean(log_mel_spectrogram, axis=1)\n",
    "\n",
    "    # Append feature and label to the lists\n",
    "    features.append(feature)\n",
    "    labels.append(label)\n",
    "\n",
    "# Convert features and labels to NumPy arrays\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Initialize a LabelEncoder object\n",
    "lb = LabelEncoder()\n",
    "\n",
    "# Encode the labels\n",
    "y_encoded = lb.fit_transform(y)\n",
    "\n",
    "# Perform one-hot encoding on the labels\n",
    "num_classes = len(np.unique(y))\n",
    "y_encoded = to_categorical(y_encoded, num_classes=num_classes)\n",
    "\n",
    "# Split between train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.25, stratify=y_encoded, random_state=42)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform both the training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the data to fit the input shape of the model\n",
    "X_train_reshaped = np.expand_dims(X_train_scaled, axis=2)\n",
    "X_test_reshaped = np.expand_dims(X_test_scaled, axis=2)\n",
    "\n",
    "# Define VQ-MAE-S ResNet model architecture with dropout regularization and early stopping\n",
    "def build_resnet_dropout(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.Conv1D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.Conv1D(256, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.Conv1D(512, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)  # Add dropout regularization\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Build and compile the model\n",
    "resnet_dropout_model = build_resnet_dropout(input_shape=X_train_reshaped.shape[1:], num_classes=num_classes)\n",
    "resnet_dropout_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "resnet_dropout_model.summary()\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with dropout regularization and early stopping\n",
    "history = resnet_dropout_model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = resnet_dropout_model.evaluate(X_test_reshaped, y_test)\n",
    "print(\"Test accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 128, 1)]          0         \n",
      "                                                                 \n",
      " conv1d_32 (Conv1D)          (None, 128, 64)           256       \n",
      "                                                                 \n",
      " max_pooling1d_18 (MaxPooli  (None, 64, 64)            0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_33 (Conv1D)          (None, 64, 128)           24704     \n",
      "                                                                 \n",
      " max_pooling1d_19 (MaxPooli  (None, 32, 128)           0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_34 (Conv1D)          (None, 32, 256)           98560     \n",
      "                                                                 \n",
      " max_pooling1d_20 (MaxPooli  (None, 16, 256)           0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_35 (Conv1D)          (None, 16, 512)           393728    \n",
      "                                                                 \n",
      " max_pooling1d_21 (MaxPooli  (None, 8, 512)            0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 512)               2097664   \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 14)                7182      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2622094 (10.00 MB)\n",
      "Trainable params: 2622094 (10.00 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "228/228 [==============================] - 5s 22ms/step - loss: 2.3221 - accuracy: 0.1959 - val_loss: 2.1271 - val_accuracy: 0.2658\n",
      "Epoch 2/100\n",
      "228/228 [==============================] - 5s 20ms/step - loss: 1.9607 - accuracy: 0.3355 - val_loss: 1.8443 - val_accuracy: 0.3797\n",
      "Epoch 3/100\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 1.7564 - accuracy: 0.3967 - val_loss: 1.7643 - val_accuracy: 0.3655\n",
      "Epoch 4/100\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 1.6149 - accuracy: 0.4324 - val_loss: 1.6838 - val_accuracy: 0.3929\n",
      "Epoch 5/100\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 1.5221 - accuracy: 0.4682 - val_loss: 1.6044 - val_accuracy: 0.4334\n",
      "Epoch 6/100\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 1.4277 - accuracy: 0.4908 - val_loss: 1.5606 - val_accuracy: 0.4389\n",
      "Epoch 7/100\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 1.3426 - accuracy: 0.5138 - val_loss: 1.5827 - val_accuracy: 0.4510\n",
      "Epoch 8/100\n",
      "228/228 [==============================] - 5s 22ms/step - loss: 1.2564 - accuracy: 0.5444 - val_loss: 1.5697 - val_accuracy: 0.4438\n",
      "Epoch 9/100\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 1.1853 - accuracy: 0.5773 - val_loss: 1.6000 - val_accuracy: 0.4586\n",
      "Epoch 10/100\n",
      "228/228 [==============================] - 5s 20ms/step - loss: 1.1053 - accuracy: 0.5954 - val_loss: 1.6296 - val_accuracy: 0.4378\n",
      "Epoch 11/100\n",
      "228/228 [==============================] - 5s 21ms/step - loss: 0.9964 - accuracy: 0.6361 - val_loss: 1.6432 - val_accuracy: 0.4537\n",
      "96/96 [==============================] - 1s 6ms/step - loss: 1.5455 - accuracy: 0.4400\n",
      "Test accuracy: 0.43998685479164124\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the dataset and perform feature extraction\n",
    "df = pd.read_csv(\"/Users/roshanscaria/Desktop/Audio Emotion/Data_path.csv\")\n",
    "\n",
    "# Define augmentation pipeline\n",
    "augment = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.02, p=0.5),\n",
    "    TimeStretch(min_rate=0.7, max_rate=1.3, p=0.5),\n",
    "    PitchShift(min_semitones=-5, max_semitones=5, p=0.5)\n",
    "])\n",
    "\n",
    "# Initialize lists to store features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Loop feature extraction over the entire dataset\n",
    "for label, path in zip(df['labels'], df['path']):\n",
    "    # Load audio file\n",
    "    X, sample_rate = librosa.load(path, res_type='kaiser_fast', duration=2.5, sr=44100, offset=0.5)\n",
    "\n",
    "    # Apply augmentation\n",
    "    X_augmented = augment(samples=X, sample_rate=sample_rate)\n",
    "\n",
    "    # Compute Mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=X_augmented, sr=sample_rate, n_mels=128)\n",
    "\n",
    "    # Convert to decibel scale (log-mel spectrogram)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Take the mean as the feature\n",
    "    feature = np.mean(log_mel_spectrogram, axis=1)\n",
    "\n",
    "    # Append feature and label to the lists\n",
    "    features.append(feature)\n",
    "    labels.append(label)\n",
    "\n",
    "# Convert features and labels to NumPy arrays\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Initialize a LabelEncoder object\n",
    "lb = LabelEncoder()\n",
    "\n",
    "# Encode the labels\n",
    "y_encoded = lb.fit_transform(y)\n",
    "\n",
    "# Perform one-hot encoding on the labels\n",
    "num_classes = len(np.unique(y))\n",
    "y_encoded = to_categorical(y_encoded, num_classes=num_classes)\n",
    "\n",
    "# Split between train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.25, stratify=y_encoded, random_state=42)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform both the training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the data to fit the input shape of the model\n",
    "X_train_reshaped = np.expand_dims(X_train_scaled, axis=2)\n",
    "X_test_reshaped = np.expand_dims(X_test_scaled, axis=2)\n",
    "\n",
    "# Define VQ-MAE-S ResNet model architecture with dropout regularization and early stopping\n",
    "def build_resnet_dropout(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv1D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.Conv1D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.Conv1D(256, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.Conv1D(512, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)  # Add dropout regularization\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Build and compile the model\n",
    "resnet_dropout_model = build_resnet_dropout(input_shape=X_train_reshaped.shape[1:], num_classes=num_classes)\n",
    "resnet_dropout_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "resnet_dropout_model.summary()\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model with dropout regularization and early stopping\n",
    "history = resnet_dropout_model.fit(X_train_reshaped, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = resnet_dropout_model.evaluate(X_test_reshaped, y_test)\n",
    "print(\"Test accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_36 (Conv1D)          (None, 128, 256)          2816      \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 128, 256)          0         \n",
      "                                                                 \n",
      " conv1d_37 (Conv1D)          (None, 128, 256)          655616    \n",
      "                                                                 \n",
      " batch_normalization_21 (Ba  (None, 128, 256)          1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 128, 256)          0         \n",
      "                                                                 \n",
      " max_pooling1d_22 (MaxPooli  (None, 42, 256)           0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_38 (Conv1D)          (None, 42, 128)           327808    \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 42, 128)           0         \n",
      "                                                                 \n",
      " conv1d_39 (Conv1D)          (None, 42, 128)           163968    \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 42, 128)           0         \n",
      "                                                                 \n",
      " conv1d_40 (Conv1D)          (None, 42, 128)           163968    \n",
      "                                                                 \n",
      " batch_normalization_22 (Ba  (None, 42, 128)           512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 42, 128)           0         \n",
      "                                                                 \n",
      " max_pooling1d_23 (MaxPooli  (None, 14, 128)           0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_41 (Conv1D)          (None, 14, 64)            81984     \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 14, 64)            0         \n",
      "                                                                 \n",
      " conv1d_42 (Conv1D)          (None, 14, 64)            41024     \n",
      "                                                                 \n",
      " activation_20 (Activation)  (None, 14, 64)            0         \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 896)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               114816    \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 14)                1806      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1555342 (5.93 MB)\n",
      "Trainable params: 1554574 (5.93 MB)\n",
      "Non-trainable params: 768 (3.00 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "228/228 [==============================] - 34s 144ms/step - loss: 2.5554 - accuracy: 0.1168 - val_loss: 2.3849 - val_accuracy: 0.1753\n",
      "Epoch 2/50\n",
      "228/228 [==============================] - 31s 136ms/step - loss: 2.3468 - accuracy: 0.1897 - val_loss: 2.3418 - val_accuracy: 0.1962\n",
      "Epoch 3/50\n",
      "228/228 [==============================] - 31s 135ms/step - loss: 2.2042 - accuracy: 0.2390 - val_loss: 2.1901 - val_accuracy: 0.2411\n",
      "Epoch 4/50\n",
      "228/228 [==============================] - 39s 171ms/step - loss: 2.0488 - accuracy: 0.2882 - val_loss: 2.0055 - val_accuracy: 0.3112\n",
      "Epoch 5/50\n",
      "228/228 [==============================] - 38s 167ms/step - loss: 1.9473 - accuracy: 0.3296 - val_loss: 1.8559 - val_accuracy: 0.3584\n",
      "Epoch 6/50\n",
      "228/228 [==============================] - 35s 154ms/step - loss: 1.8750 - accuracy: 0.3514 - val_loss: 1.8111 - val_accuracy: 0.3638\n",
      "Epoch 7/50\n",
      "228/228 [==============================] - 35s 155ms/step - loss: 1.7873 - accuracy: 0.3906 - val_loss: 1.8597 - val_accuracy: 0.3512\n",
      "Epoch 8/50\n",
      "228/228 [==============================] - 33s 146ms/step - loss: 1.7282 - accuracy: 0.4016 - val_loss: 1.7386 - val_accuracy: 0.3901\n",
      "Epoch 9/50\n",
      "228/228 [==============================] - 35s 152ms/step - loss: 1.6724 - accuracy: 0.4153 - val_loss: 1.7971 - val_accuracy: 0.3847\n",
      "Epoch 10/50\n",
      "228/228 [==============================] - 33s 146ms/step - loss: 1.6119 - accuracy: 0.4331 - val_loss: 1.6572 - val_accuracy: 0.4263\n",
      "Epoch 11/50\n",
      "228/228 [==============================] - 35s 153ms/step - loss: 1.5596 - accuracy: 0.4575 - val_loss: 1.6429 - val_accuracy: 0.4115\n",
      "Epoch 12/50\n",
      "228/228 [==============================] - 34s 149ms/step - loss: 1.5099 - accuracy: 0.4613 - val_loss: 1.6129 - val_accuracy: 0.4230\n",
      "Epoch 13/50\n",
      "228/228 [==============================] - 33s 143ms/step - loss: 1.4741 - accuracy: 0.4715 - val_loss: 1.6031 - val_accuracy: 0.4351\n",
      "Epoch 14/50\n",
      "228/228 [==============================] - 33s 146ms/step - loss: 1.4341 - accuracy: 0.4925 - val_loss: 1.5886 - val_accuracy: 0.4296\n",
      "Epoch 15/50\n",
      "228/228 [==============================] - 32s 143ms/step - loss: 1.3754 - accuracy: 0.5071 - val_loss: 1.6333 - val_accuracy: 0.4389\n",
      "Epoch 16/50\n",
      "228/228 [==============================] - 33s 143ms/step - loss: 1.3244 - accuracy: 0.5218 - val_loss: 1.6366 - val_accuracy: 0.4290\n",
      "Epoch 17/50\n",
      "228/228 [==============================] - 34s 148ms/step - loss: 1.3010 - accuracy: 0.5232 - val_loss: 1.6458 - val_accuracy: 0.4411\n",
      "Epoch 18/50\n",
      "228/228 [==============================] - 33s 143ms/step - loss: 1.2496 - accuracy: 0.5463 - val_loss: 1.6567 - val_accuracy: 0.4274\n",
      "Epoch 19/50\n",
      "228/228 [==============================] - 33s 144ms/step - loss: 1.2170 - accuracy: 0.5604 - val_loss: 1.5827 - val_accuracy: 0.4449\n",
      "Epoch 20/50\n",
      "228/228 [==============================] - 33s 143ms/step - loss: 1.1873 - accuracy: 0.5563 - val_loss: 1.7011 - val_accuracy: 0.4395\n",
      "Epoch 21/50\n",
      "228/228 [==============================] - 36s 159ms/step - loss: 1.1687 - accuracy: 0.5702 - val_loss: 1.6989 - val_accuracy: 0.4411\n",
      "Epoch 22/50\n",
      "228/228 [==============================] - 35s 153ms/step - loss: 1.1018 - accuracy: 0.6007 - val_loss: 1.7329 - val_accuracy: 0.4542\n",
      "Epoch 23/50\n",
      "228/228 [==============================] - 33s 145ms/step - loss: 1.0665 - accuracy: 0.6054 - val_loss: 1.6293 - val_accuracy: 0.4586\n",
      "Epoch 24/50\n",
      "228/228 [==============================] - 32s 140ms/step - loss: 1.0315 - accuracy: 0.6121 - val_loss: 1.7222 - val_accuracy: 0.4548\n",
      "Epoch 25/50\n",
      "228/228 [==============================] - 33s 143ms/step - loss: 0.9945 - accuracy: 0.6305 - val_loss: 1.7111 - val_accuracy: 0.4630\n",
      "Epoch 26/50\n",
      "228/228 [==============================] - 32s 142ms/step - loss: 0.9532 - accuracy: 0.6447 - val_loss: 1.8304 - val_accuracy: 0.4477\n",
      "Epoch 27/50\n",
      "228/228 [==============================] - 32s 139ms/step - loss: 0.9192 - accuracy: 0.6542 - val_loss: 1.8227 - val_accuracy: 0.4542\n",
      "Epoch 28/50\n",
      "228/228 [==============================] - 33s 144ms/step - loss: 0.8776 - accuracy: 0.6695 - val_loss: 1.8637 - val_accuracy: 0.4477\n",
      "Epoch 29/50\n",
      "228/228 [==============================] - 32s 141ms/step - loss: 0.8356 - accuracy: 0.6848 - val_loss: 1.9670 - val_accuracy: 0.4488\n",
      "Epoch 30/50\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.8254 - accuracy: 0.6967 - val_loss: 1.8806 - val_accuracy: 0.4586\n",
      "Epoch 31/50\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 0.7778 - accuracy: 0.7111 - val_loss: 2.0370 - val_accuracy: 0.4548\n",
      "Epoch 32/50\n",
      "228/228 [==============================] - 35s 154ms/step - loss: 0.7577 - accuracy: 0.7138 - val_loss: 2.0571 - val_accuracy: 0.4373\n",
      "Epoch 33/50\n",
      "228/228 [==============================] - 40s 175ms/step - loss: 0.7414 - accuracy: 0.7266 - val_loss: 2.0998 - val_accuracy: 0.4395\n",
      "Epoch 34/50\n",
      "228/228 [==============================] - 37s 161ms/step - loss: 0.7337 - accuracy: 0.7245 - val_loss: 2.1682 - val_accuracy: 0.4395\n",
      "Epoch 35/50\n",
      "228/228 [==============================] - 36s 159ms/step - loss: 0.6962 - accuracy: 0.7411 - val_loss: 2.0662 - val_accuracy: 0.4510\n",
      "Epoch 36/50\n",
      "228/228 [==============================] - 34s 150ms/step - loss: 0.6471 - accuracy: 0.7556 - val_loss: 2.2547 - val_accuracy: 0.4493\n",
      "Epoch 37/50\n",
      "228/228 [==============================] - 35s 153ms/step - loss: 0.6412 - accuracy: 0.7560 - val_loss: 2.2447 - val_accuracy: 0.4329\n",
      "Epoch 38/50\n",
      "228/228 [==============================] - 40s 174ms/step - loss: 0.6022 - accuracy: 0.7718 - val_loss: 2.4756 - val_accuracy: 0.4416\n",
      "Epoch 39/50\n",
      "228/228 [==============================] - 35s 154ms/step - loss: 0.5931 - accuracy: 0.7785 - val_loss: 2.3625 - val_accuracy: 0.4515\n",
      "Epoch 40/50\n",
      "228/228 [==============================] - 36s 159ms/step - loss: 0.5562 - accuracy: 0.7867 - val_loss: 2.5917 - val_accuracy: 0.4482\n",
      "Epoch 41/50\n",
      "228/228 [==============================] - 37s 160ms/step - loss: 0.5491 - accuracy: 0.7921 - val_loss: 2.5371 - val_accuracy: 0.4405\n",
      "Epoch 42/50\n",
      "228/228 [==============================] - 37s 161ms/step - loss: 0.5299 - accuracy: 0.8010 - val_loss: 2.3965 - val_accuracy: 0.4455\n",
      "Epoch 43/50\n",
      "228/228 [==============================] - 39s 169ms/step - loss: 0.5159 - accuracy: 0.8076 - val_loss: 2.6145 - val_accuracy: 0.4471\n",
      "Epoch 44/50\n",
      "228/228 [==============================] - 36s 157ms/step - loss: 0.5173 - accuracy: 0.8141 - val_loss: 2.6081 - val_accuracy: 0.4553\n",
      "Epoch 45/50\n",
      "228/228 [==============================] - 40s 176ms/step - loss: 0.4985 - accuracy: 0.8169 - val_loss: 2.8266 - val_accuracy: 0.4411\n",
      "Epoch 46/50\n",
      "228/228 [==============================] - 34s 148ms/step - loss: 0.4463 - accuracy: 0.8335 - val_loss: 2.7381 - val_accuracy: 0.4526\n",
      "Epoch 47/50\n",
      "228/228 [==============================] - 36s 157ms/step - loss: 0.4473 - accuracy: 0.8343 - val_loss: 2.9840 - val_accuracy: 0.4537\n",
      "Epoch 48/50\n",
      "228/228 [==============================] - 35s 154ms/step - loss: 0.4312 - accuracy: 0.8399 - val_loss: 3.0013 - val_accuracy: 0.4373\n",
      "Epoch 49/50\n",
      "228/228 [==============================] - 36s 157ms/step - loss: 0.4473 - accuracy: 0.8355 - val_loss: 3.1399 - val_accuracy: 0.4263\n",
      "Epoch 50/50\n",
      "228/228 [==============================] - 41s 179ms/step - loss: 0.4135 - accuracy: 0.8491 - val_loss: 3.0002 - val_accuracy: 0.4449\n",
      "96/96 [==============================] - 4s 44ms/step - loss: 3.1444 - accuracy: 0.4476\n",
      "Test accuracy: 0.4475501477718353\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the dataset and perform feature extraction\n",
    "df = pd.read_csv(\"/Users/roshanscaria/Desktop/Audio Emotion/Data_path.csv\")\n",
    "\n",
    "# Define augmentation pipeline\n",
    "augment = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.02, p=0.5),\n",
    "    TimeStretch(min_rate=0.7, max_rate=1.3, p=0.5),\n",
    "    PitchShift(min_semitones=-5, max_semitones=5, p=0.5)\n",
    "])\n",
    "\n",
    "# Initialize lists to store features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Loop feature extraction over the entire dataset\n",
    "for label, path in zip(df['labels'], df['path']):\n",
    "    # Load audio file\n",
    "    X, sample_rate = librosa.load(path, res_type='kaiser_fast', duration=2.5, sr=44100, offset=0.5)\n",
    "\n",
    "    # Apply augmentation\n",
    "    X_augmented = augment(samples=X, sample_rate=sample_rate)\n",
    "\n",
    "    # Compute Mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=X_augmented, sr=sample_rate, n_mels=128)\n",
    "\n",
    "    # Convert to decibel scale (log-mel spectrogram)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Take the mean as the feature\n",
    "    feature = np.mean(log_mel_spectrogram, axis=1)\n",
    "\n",
    "    # Append feature and label to the lists\n",
    "    features.append(feature)\n",
    "    labels.append(label)\n",
    "\n",
    "# Convert features and labels to NumPy arrays\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Initialize a LabelEncoder object\n",
    "lb = LabelEncoder()\n",
    "\n",
    "# Encode the labels\n",
    "y_encoded = lb.fit_transform(y)\n",
    "\n",
    "# Perform one-hot encoding on the labels\n",
    "num_classes = len(np.unique(y))\n",
    "y_encoded = to_categorical(y_encoded, num_classes=num_classes)\n",
    "\n",
    "# Split between train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.25, stratify=y_encoded, random_state=42)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform both the training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the data to fit the input shape of the model\n",
    "X_train_reshaped = np.expand_dims(X_train_scaled, axis=2)\n",
    "X_test_reshaped = np.expand_dims(X_test_scaled, axis=2)\n",
    "\n",
    "# Define the modified model\n",
    "model = models.Sequential([\n",
    "    layers.Conv1D(256, 10, padding='same', input_shape=(X_train_reshaped.shape[1], 1)),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Conv1D(256, 10, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(128, 10, padding='same'),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Conv1D(128, 10, padding='same'),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Conv1D(128, 10, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(64, 10, padding='same'),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Conv1D(64, 10, padding='same'),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Use Adam optimizer with a lower learning rate\n",
    "opt = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_reshaped, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(X_test_reshaped, y_test)\n",
    "print(\"Test accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_43 (Conv1D)          (None, 128, 256)          2816      \n",
      "                                                                 \n",
      " activation_21 (Activation)  (None, 128, 256)          0         \n",
      "                                                                 \n",
      " conv1d_44 (Conv1D)          (None, 128, 256)          655616    \n",
      "                                                                 \n",
      " batch_normalization_23 (Ba  (None, 128, 256)          1024      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_22 (Activation)  (None, 128, 256)          0         \n",
      "                                                                 \n",
      " max_pooling1d_24 (MaxPooli  (None, 42, 256)           0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_45 (Conv1D)          (None, 42, 128)           327808    \n",
      "                                                                 \n",
      " activation_23 (Activation)  (None, 42, 128)           0         \n",
      "                                                                 \n",
      " conv1d_46 (Conv1D)          (None, 42, 128)           163968    \n",
      "                                                                 \n",
      " activation_24 (Activation)  (None, 42, 128)           0         \n",
      "                                                                 \n",
      " conv1d_47 (Conv1D)          (None, 42, 128)           163968    \n",
      "                                                                 \n",
      " batch_normalization_24 (Ba  (None, 42, 128)           512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_25 (Activation)  (None, 42, 128)           0         \n",
      "                                                                 \n",
      " max_pooling1d_25 (MaxPooli  (None, 14, 128)           0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_48 (Conv1D)          (None, 14, 64)            81984     \n",
      "                                                                 \n",
      " activation_26 (Activation)  (None, 14, 64)            0         \n",
      "                                                                 \n",
      " conv1d_49 (Conv1D)          (None, 14, 64)            41024     \n",
      "                                                                 \n",
      " activation_27 (Activation)  (None, 14, 64)            0         \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 896)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               114816    \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 14)                1806      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1555342 (5.93 MB)\n",
      "Trainable params: 1554574 (5.93 MB)\n",
      "Non-trainable params: 768 (3.00 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "228/228 [==============================] - 30s 126ms/step - loss: 3.1879 - accuracy: 0.1118 - val_loss: 2.5520 - val_accuracy: 0.1660\n",
      "Epoch 2/50\n",
      "228/228 [==============================] - 31s 137ms/step - loss: 2.4500 - accuracy: 0.1806 - val_loss: 2.3563 - val_accuracy: 0.1973\n",
      "Epoch 3/50\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 2.2678 - accuracy: 0.2436 - val_loss: 2.6394 - val_accuracy: 0.1441\n",
      "Epoch 4/50\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 2.1388 - accuracy: 0.2880 - val_loss: 2.0085 - val_accuracy: 0.3222\n",
      "Epoch 5/50\n",
      "228/228 [==============================] - 31s 138ms/step - loss: 2.0122 - accuracy: 0.3363 - val_loss: 1.9167 - val_accuracy: 0.3551\n",
      "Epoch 6/50\n",
      "228/228 [==============================] - 30s 133ms/step - loss: 1.9415 - accuracy: 0.3599 - val_loss: 1.8379 - val_accuracy: 0.3852\n",
      "Epoch 7/50\n",
      "228/228 [==============================] - 34s 148ms/step - loss: 1.8435 - accuracy: 0.3912 - val_loss: 1.8147 - val_accuracy: 0.4016\n",
      "Epoch 8/50\n",
      "228/228 [==============================] - 32s 142ms/step - loss: 1.7956 - accuracy: 0.4057 - val_loss: 1.7833 - val_accuracy: 0.4126\n",
      "Epoch 9/50\n",
      "228/228 [==============================] - 34s 151ms/step - loss: 1.7422 - accuracy: 0.4271 - val_loss: 1.7256 - val_accuracy: 0.4241\n",
      "Epoch 10/50\n",
      "228/228 [==============================] - 32s 141ms/step - loss: 1.6846 - accuracy: 0.4389 - val_loss: 1.6748 - val_accuracy: 0.4422\n",
      "Epoch 11/50\n",
      "228/228 [==============================] - 31s 134ms/step - loss: 1.6518 - accuracy: 0.4522 - val_loss: 1.6862 - val_accuracy: 0.4384\n",
      "Epoch 12/50\n",
      "228/228 [==============================] - 30s 133ms/step - loss: 1.6044 - accuracy: 0.4612 - val_loss: 1.6888 - val_accuracy: 0.4279\n",
      "Epoch 13/50\n",
      "228/228 [==============================] - 30s 132ms/step - loss: 1.5704 - accuracy: 0.4759 - val_loss: 1.6626 - val_accuracy: 0.4362\n",
      "Epoch 14/50\n",
      "228/228 [==============================] - 30s 131ms/step - loss: 1.5251 - accuracy: 0.4919 - val_loss: 1.6442 - val_accuracy: 0.4570\n",
      "Epoch 15/50\n",
      "228/228 [==============================] - 31s 137ms/step - loss: 1.5003 - accuracy: 0.4988 - val_loss: 1.6359 - val_accuracy: 0.4460\n",
      "Epoch 16/50\n",
      "228/228 [==============================] - 32s 139ms/step - loss: 1.4617 - accuracy: 0.5056 - val_loss: 1.6434 - val_accuracy: 0.4532\n",
      "Epoch 17/50\n",
      "228/228 [==============================] - 30s 132ms/step - loss: 1.4249 - accuracy: 0.5189 - val_loss: 1.6319 - val_accuracy: 0.4592\n",
      "Epoch 18/50\n",
      "228/228 [==============================] - 31s 135ms/step - loss: 1.3962 - accuracy: 0.5339 - val_loss: 1.6470 - val_accuracy: 0.4504\n",
      "Epoch 19/50\n",
      "228/228 [==============================] - 30s 130ms/step - loss: 1.3640 - accuracy: 0.5419 - val_loss: 1.6864 - val_accuracy: 0.4427\n",
      "Epoch 20/50\n",
      "228/228 [==============================] - 30s 134ms/step - loss: 1.3440 - accuracy: 0.5491 - val_loss: 1.6418 - val_accuracy: 0.4570\n",
      "Epoch 21/50\n",
      "228/228 [==============================] - 30s 131ms/step - loss: 1.2840 - accuracy: 0.5692 - val_loss: 1.6658 - val_accuracy: 0.4647\n",
      "Epoch 22/50\n",
      "228/228 [==============================] - 31s 135ms/step - loss: 1.2587 - accuracy: 0.5691 - val_loss: 1.6820 - val_accuracy: 0.4532\n",
      "Epoch 23/50\n",
      "228/228 [==============================] - 30s 130ms/step - loss: 1.2341 - accuracy: 0.5862 - val_loss: 1.7273 - val_accuracy: 0.4477\n",
      "Epoch 24/50\n",
      "228/228 [==============================] - 30s 133ms/step - loss: 1.1880 - accuracy: 0.6040 - val_loss: 1.7251 - val_accuracy: 0.4679\n",
      "Epoch 25/50\n",
      "228/228 [==============================] - 30s 132ms/step - loss: 1.1397 - accuracy: 0.6192 - val_loss: 1.8536 - val_accuracy: 0.4471\n",
      "Epoch 26/50\n",
      "228/228 [==============================] - 31s 135ms/step - loss: 1.1250 - accuracy: 0.6264 - val_loss: 1.8479 - val_accuracy: 0.4570\n",
      "Epoch 27/50\n",
      "228/228 [==============================] - 30s 132ms/step - loss: 1.0704 - accuracy: 0.6476 - val_loss: 1.8469 - val_accuracy: 0.4427\n",
      "96/96 [==============================] - 3s 34ms/step - loss: 1.6571 - accuracy: 0.4456\n",
      "Test accuracy: 0.44557711482048035\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift\n",
    "\n",
    "# Load the dataset and perform feature extraction\n",
    "df = pd.read_csv(\"/Users/roshanscaria/Desktop/Audio Emotion/Data_path.csv\")\n",
    "\n",
    "# Define augmentation pipeline\n",
    "augment = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.02, p=0.5),\n",
    "    TimeStretch(min_rate=0.7, max_rate=1.3, p=0.5),\n",
    "    PitchShift(min_semitones=-5, max_semitones=5, p=0.5)\n",
    "])\n",
    "\n",
    "# Initialize lists to store features and labels\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# Loop feature extraction over the entire dataset\n",
    "for label, path in zip(df['labels'], df['path']):\n",
    "    # Load audio file\n",
    "    X, sample_rate = librosa.load(path, res_type='kaiser_fast', duration=2.5, sr=44100, offset=0.5)\n",
    "\n",
    "    # Apply augmentation\n",
    "    X_augmented = augment(samples=X, sample_rate=sample_rate)\n",
    "\n",
    "    # Compute Mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=X_augmented, sr=sample_rate, n_mels=128)\n",
    "\n",
    "    # Convert to decibel scale (log-mel spectrogram)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Take the mean as the feature\n",
    "    feature = np.mean(log_mel_spectrogram, axis=1)\n",
    "\n",
    "    # Append feature and label to the lists\n",
    "    features.append(feature)\n",
    "    labels.append(label)\n",
    "\n",
    "# Convert features and labels to NumPy arrays\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Initialize a LabelEncoder object\n",
    "lb = LabelEncoder()\n",
    "\n",
    "# Encode the labels\n",
    "y_encoded = lb.fit_transform(y)\n",
    "\n",
    "# Perform one-hot encoding on the labels\n",
    "num_classes = len(np.unique(y))\n",
    "y_encoded = to_categorical(y_encoded, num_classes=num_classes)\n",
    "\n",
    "# Split between train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.25, stratify=y_encoded, random_state=42)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform both the training and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the data to fit the input shape of the model\n",
    "X_train_reshaped = np.expand_dims(X_train_scaled, axis=2)\n",
    "X_test_reshaped = np.expand_dims(X_test_scaled, axis=2)\n",
    "\n",
    "# Define the modified model\n",
    "model = models.Sequential([\n",
    "    layers.Conv1D(256, 10, padding='same', input_shape=(X_train_reshaped.shape[1], 1)),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Conv1D(256, 10, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(128, 10, padding='same'),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Conv1D(128, 10, padding='same'),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Conv1D(128, 10, padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.MaxPooling1D(pool_size=3),\n",
    "    layers.Conv1D(64, 10, padding='same'),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Conv1D(64, 10, padding='same'),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    layers.Dropout(0.6),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Use Adam optimizer with a lower learning rate\n",
    "opt = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Implement Early Stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train_reshaped, y_train, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_split=0.2, \n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(X_test_reshaped, y_test)\n",
    "print(\"Test accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
